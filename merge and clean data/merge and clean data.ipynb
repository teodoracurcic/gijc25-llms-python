{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f749db8",
   "metadata": {},
   "source": [
    "# Merge and clean data in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816200e",
   "metadata": {},
   "source": [
    "In this notebook, we‚Äôll walk through the process of **combining, cleaning, and preparing real-world datasets** for analysis and storytelling using only a few lines of Python.\n",
    "\n",
    "Imagine you‚Äôve collected price data for various grocery items (like rice, flour, or oil) from a website that gathers daily prices from multiple stores across the country, one dataset per day. After a week, you now have **10 CSV files**, each containing around **20,000 daily entries**.\n",
    "\n",
    "Our goal is to:\n",
    "1. **Merge** all CSV files into a single dataset.  \n",
    "2. **Clean and standardize** the data by fixing inconsistent units, product names, and missing values.  \n",
    "3. **Prepare** the data for analysis and visualization in later notebooks.\n",
    "\n",
    "This exercise shows how Python and AI tools like ChatGPT can make complex data work simple, even if you‚Äôve never coded before.\n",
    "\n",
    "At the beginning, we‚Äôll install or import the **libraries** we need to work with in Jupyter Notebook. Libraries are collections of pre-written code that make it easier to perform specific tasks, like reading files, cleaning data, or creating charts - without writing everything from scratch. If you haven‚Äôt used a library before, you can install it using the `pip install` command. If you already have it, you just need to `import` it into your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661d1fc",
   "metadata": {},
   "source": [
    "### Step 1: Loading CSV Files from a Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c732fe1f",
   "metadata": {},
   "source": [
    "We know that the folder is on our computer and that it contains several CSV files we want to merge into one. We also know that we are using either **Windows** or a **MacBook** (the process isn‚Äôt identical, which is always good to point out). That information is enough for us to start:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a72694",
   "metadata": {},
   "source": [
    "*I have a folder on my MacBook that contains several CSV files. I want to load them into a Jupyter Notebook and merge them into one file. I‚Äôm a beginner, so I‚Äôd like the code to be simple and easy to understand. Start from the beginning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a3e09",
   "metadata": {},
   "source": [
    "This is what we got:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855edce8",
   "metadata": {},
   "source": [
    "*ü™Ñ Step 1: Import the libraries*\n",
    "\n",
    "*Libraries are pre-written bits of code that make your life easier. We‚Äôll use:*\n",
    "\n",
    "*pandas ‚Üí for working with data*\n",
    "\n",
    "*glob ‚Üí for finding all CSV files in a folder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504585dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91015150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd      # pandas = library for working with data (tables, CSVs, etc.)\n",
    "import glob              # glob = helps find files matching a pattern (e.g., *.csv)\n",
    "import os                # os = provides functions to interact with the operating system (paths, folders, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41616967",
   "metadata": {},
   "source": [
    "*üìÅ Step 2: Set the path to your folder*\n",
    "\n",
    "*Tell Python where your CSV files are located. On a MacBook, paths usually look like this: /Users/yourname/Documents/data. To avoid typing long paths manually, you can go to Finder to copy its path.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a path to the \"data\" folder inside the current working directory\n",
    "path = os.path.join(os.getcwd(), \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba57150",
   "metadata": {},
   "source": [
    "*üß© Step 3: Find and read all CSV files*\n",
    "\n",
    "*Use the glob library to get all the CSV filenames in that folder, then read them one by one with pandas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all CSV files in the folder\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Check how many files were found\n",
    "print(\"Number of files found:\", len(csv_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be852cb",
   "metadata": {},
   "source": [
    "*üß∫ Step 4: Merge all files into one DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e92439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and combine all CSVs into one DataFrame\n",
    "all_data = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "\n",
    "# Take a look at the first few rows\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0511ee6",
   "metadata": {},
   "source": [
    "*üíæ Step 5: (Optional) Save the merged file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined dataset to a new CSV file\n",
    "all_data.to_csv(\"merged_data.csv\", index=False)\n",
    "print(\"Merged file saved as merged_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f14f3",
   "metadata": {},
   "source": [
    "### Step 2: Analyze our merged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ddfd26",
   "metadata": {},
   "source": [
    "Now that we‚Äôve merged all our CSV files into one dataset, let‚Äôs take a look at what we have.\n",
    "\n",
    "We‚Äôll start with some basic analysis:\n",
    "1. Check how many total rows (entries) there are in the dataset.  \n",
    "2. Count how many unique categories exist in the `category` column.  \n",
    "3. Calculate the minimum, maximum, and average values for the `discount` column.  \n",
    "4. Find the cheapest and most expensive products based on `price_rsd`, showing the entire row for each.  \n",
    "\n",
    "*Now that we‚Äôve merged all our CSV files into one dataset, I want to explore it using Python and pandas by checking how many total rows the dataset has, how many unique categories exist in the category column, calculating the minimum, maximum, and average values in the discount column, and finding the cheapest and most expensive products based on the price_rsd column while displaying the full row for each.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6038d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check how many total rows (entries) are in the dataset\n",
    "total_rows = len(all_data)\n",
    "print(\"Total number of rows:\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe29939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Count how many unique categories exist in the 'category' column\n",
    "unique_categories = all_data['category'].nunique()\n",
    "print(\"Number of unique categories:\", unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculate the minimum, maximum, and average discount values\n",
    "min_discount = all_data['discount'].min()\n",
    "max_discount = all_data['discount'].max()\n",
    "avg_discount = all_data['discount'].mean()\n",
    "print(f\"Discount - Min: {min_discount}, Max: {max_discount}, Average: {avg_discount:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f19aec",
   "metadata": {},
   "source": [
    "Ooops, an error. Let's copy it and see what that means.\n",
    "\n",
    "*The error happens because the discount column contains both numbers and text (like \"15%\" or \"N/A\"), so we need to clean and convert all values to numeric before calculating min, max, or average. You can fix it by cleaning the discount column and converting all values to numbers before doing any calculations. Here‚Äôs the code you can use:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207083cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert discount values to numeric\n",
    "all_data['discount'] = (\n",
    "    all_data['discount']\n",
    "    .astype(str)                     # make sure all values are strings\n",
    "    .str.replace('%', '', regex=False)  # remove percentage signs\n",
    "    .str.strip()                     # remove extra spaces\n",
    ")\n",
    "\n",
    "# Convert cleaned strings to numeric, turning errors into NaN\n",
    "all_data['discount'] = pd.to_numeric(all_data['discount'], errors='coerce')\n",
    "\n",
    "# Now calculate min, max, and average safely\n",
    "min_discount = all_data['discount'].min()\n",
    "max_discount = all_data['discount'].max()\n",
    "avg_discount = all_data['discount'].mean()\n",
    "\n",
    "print(f\"Discount - Min: {min_discount}, Max: {max_discount}, Average: {avg_discount:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a1cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Find the cheapest and most expensive products based on 'price_rsd'\n",
    "# and show the full row for each\n",
    "cheapest_product = all_data.loc[all_data['price_rsd'].idxmin()]\n",
    "most_expensive_product = all_data.loc[all_data['price_rsd'].idxmax()]\n",
    "\n",
    "print(\"\\nCheapest product:\")\n",
    "print(cheapest_product)\n",
    "\n",
    "print(\"\\nMost expensive product:\")\n",
    "print(most_expensive_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d9c5e",
   "metadata": {},
   "source": [
    "Now that I‚Äôve explored the dataset, I want to see a list of all unique categories and a list of all unique stores (from the shop column) to better understand what‚Äôs included in my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all unique categories\n",
    "if 'category' in all_data.columns:\n",
    "    print(\"List of all categories:\")\n",
    "    print(all_data['category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ce792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all unique stores (shop column)\n",
    "if 'shop' in all_data.columns:\n",
    "    print(\"\\nList of all stores:\")\n",
    "    print(all_data['shop'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d00bc",
   "metadata": {},
   "source": [
    "Now I want to find out which product is the most expensive in each store and which is the most expensive in each category.\n",
    "I‚Äôd like to see the full rows for those products so I can identify them clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f315ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most expensive product in each category\n",
    "if 'category' in all_data.columns and 'price_rsd' in all_data.columns:\n",
    "    most_expensive_per_category = all_data.loc[all_data.groupby('category')['price_rsd'].idxmax()]\n",
    "    print(\"\\nMost expensive product in each category:\")\n",
    "    display(most_expensive_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed200522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most expensive product in each store\n",
    "if 'shop' in all_data.columns and 'price_rsd' in all_data.columns:\n",
    "    most_expensive_per_store = all_data.loc[all_data.groupby('shop')['price_rsd'].idxmax()]\n",
    "    print(\"Most expensive product in each store:\")\n",
    "    display(most_expensive_per_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838614f6",
   "metadata": {},
   "source": [
    "### Step 3: Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a2add4",
   "metadata": {},
   "source": [
    "Before cleaning the data, I want to check the data types of all columns in my dataset. This helps me understand which columns contain numbers, text, or dates - and which ones might need to be converted or cleaned. We can even ask for an explanation of data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef973f",
   "metadata": {},
   "source": [
    "### üß© Data Types in Python (pandas)\n",
    "\n",
    "Each column in your dataset has a **data type**, which tells Python what kind of data it contains.  \n",
    "This matters because you can only do certain operations with certain types.\n",
    "\n",
    "| Data Type | Meaning | Example |\n",
    "|------------|----------|----------|\n",
    "| `object` | Text (strings) | `\"rice\"`, `\"Store A\"` |\n",
    "| `int64` | Whole numbers | `10`, `2500` |\n",
    "| `float64` | Decimal numbers | `12.5`, `3.14` |\n",
    "| `bool` | True/False values | `True`, `False` |\n",
    "| `datetime64` | Dates and times | `2025-11-09` |\n",
    "\n",
    "‚úÖ *Knowing data types helps you clean and analyze data correctly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dffc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types for all columns\n",
    "print(\"Data types of all columns:\\n\")\n",
    "print(all_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f90387e",
   "metadata": {},
   "source": [
    "*Now I want to label specific products within a broader category: for example, identify all items that are Trapist cheese. In my dataset, everything in the category column labeled ‚Äúsirevi‚Äù (cheese) that has the word ‚Äútrapist‚Äù in the product_title (regardless of lowercase or uppercase letters) should be tagged as ‚Äútrapist sir‚Äù in a new column called product. This is just an example, but this approach lets us group similar products (like all Trapist cheeses) together even if they come in different sizes, packages, or brands.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'product' with default empty values\n",
    "all_data['product'] = \"\"\n",
    "\n",
    "# Label all Trapist cheeses within the 'sirevi' category\n",
    "mask = (all_data['category'].str.lower() == 'sirevi') & (all_data['product_title'].str.lower().str.contains('trapist'))\n",
    "\n",
    "all_data.loc[mask, 'product'] = 'trapist sir'\n",
    "\n",
    "# Check how many were labeled\n",
    "print(\"Number of 'trapist sir' products found:\", all_data['product'].value_counts().get('trapist sir', 0))\n",
    "\n",
    "# (Optional) Preview all unique Trapist product names\n",
    "unique_trapist_products = all_data.loc[all_data['product'] == 'trapist sir', 'product_title'].unique()\n",
    "\n",
    "print(\"Unique Trapist products found:\")\n",
    "for p in unique_trapist_products:\n",
    "    print(\"-\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994914a6",
   "metadata": {},
   "source": [
    "Now I want to extract the quantity (weight) information from the product titles. In my dataset, product names often include the package size - for example, ‚ÄúTrapist BISER 45%mm 250g‚Äù or ‚ÄúTrapist PILOS 300g‚Äù. I want to automatically detect and separate that number and unit (like 250 and g) into new columns called quantity_value and quantity_unit. This will allow me to later analyze price per package or even calculate price per kilogram across different products. I've got this explanation and a code:\n",
    "\n",
    "*Extracting quantity with `re` We‚Äôll use Python‚Äôs built-in **`re` (regular expressions)** module to find patterns like ‚Äú250g‚Äù or ‚Äú1kg‚Äù inside product names. It helps us automatically detect numbers and units (like grams or kilograms) even if they‚Äôre written differently.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367448ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract quantity (number and unit) from product titles using regex\n",
    "def extract_quantity(title):\n",
    "    match = re.search(r'(\\d+(?:[.,]\\d+)?)(\\s*)(kg|g|l|ml)', str(title).lower())\n",
    "    if match:\n",
    "        value = match.group(1).replace(',', '.')\n",
    "        unit = match.group(3)\n",
    "        return float(value), unit\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Apply the function to create two new columns\n",
    "all_data['quantity_value'], all_data['quantity_unit'] = zip(*all_data['product_title'].apply(extract_quantity))\n",
    "\n",
    "# Preview full dataset for labeled products\n",
    "all_data[all_data['product'] == 'trapist sir'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bbe076",
   "metadata": {},
   "source": [
    "Now I want to calculate the average price for ‚Äútrapist sir‚Äù products that have a package size of 250g. In my dataset, I want to group these results by date, so I can see how the average price changes over time for that specific product size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b10208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if it's not installed, we need to do this first\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3275498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only \"trapist sir\" products with a quantity of 1kg\n",
    "trapist_1kg = all_data[(all_data['product'] == 'trapist sir') & (all_data['quantity_value'] == 1)]\n",
    "\n",
    "# Group by date and calculate the average price\n",
    "avg_price_by_date = trapist_1kg.groupby('date')['price_rsd'].mean().reset_index()\n",
    "\n",
    "# Show results\n",
    "print(avg_price_by_date)\n",
    "\n",
    "# (Optional) visualize price trend over time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(avg_price_by_date['date'], avg_price_by_date['price_rsd'], marker='o')\n",
    "plt.title('Average Price of Trapist Sir 1kg Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Price (RSD)')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812aa23f",
   "metadata": {},
   "source": [
    "We can apply the same approach to other products as well to track how their prices changed over time. Later, we can also explore questions like which day the 1 kg Trapist cheese was the most or least expensive, and in which store it was sold at the best price: this code works even with much larger datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
